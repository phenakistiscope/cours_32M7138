{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1893921d-15db-45d8-a0a0-ac4eb5ac4a26",
   "metadata": {},
   "source": [
    "# **Manipuler une image grâce au code: partie 2**\n",
    "\n",
    "## Introduction à l'analyse des images - 32M7132\n",
    "\n",
    "*Adrien Jeanrenaud (adrien.jeanrenaud@unige.ch)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a199e71-33b1-4d54-b71f-f0b3d69c4b23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Manipulations partie 2</b> : dans la poursuite de nos explorations des images en tant que matrice, nous allons voir comment appliquer des algorithmes, plus ou moins complexes, aux images.\n",
    "</div>\n",
    "\n",
    "## **Plan du cours**\n",
    "\n",
    "> **Corriger les images**\n",
    "> * L'exposition\n",
    "> * Le contraste\n",
    "\n",
    "> **Les filtres et leurs applications**\n",
    "> * Qu'est-ce qu'un filtre?\n",
    "> * Attention aux bords\n",
    "> * Du filtre à l'image\n",
    "> * Des filtres pour réduire le bruit\n",
    "> * Des filtres pour détecter les contours\n",
    "\n",
    "> **La détection sémantique grâce à l'apprentissage profond**\n",
    "> * Qu'est-ce que l'apprentissage profond?\n",
    "> * Qu'est-ce qu'un algorithme d'apprentissage profond?\n",
    "> * Appliquer un algorithme d'apprentissage profond à une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f3918-59c6-4985-a10b-32e47c506144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les librairies nécessaires\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28981df7-73e7-45df-97f3-457d915faddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer une image\n",
    "\n",
    "image = \"images/japanPoster.jpg\"\n",
    "img = cv2.imread(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513741af-cc0a-4810-8047-1ceb8dd41cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser\n",
    "\n",
    "color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01379846-45a5-464d-9980-7e059a0b2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# et en valeurs de gris\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(gray, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a7e2c-05e8-4952-a4a1-85059e150156",
   "metadata": {},
   "source": [
    "## **1. Corriger les images**\n",
    "\n",
    "### 1.1 L'exposition\n",
    "\n",
    "Il est possible de corriger l'exposition, c'est à dire d'éclaircir ou d'assombrir une image. En utilisant le correction gamma, on contrôle la luminosité en changeant les rations RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff34de3-b33c-4b1f-aec2-96f410570c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "\n",
    "?exposure.adjust_gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b035ea-5bf3-422b-8274-8bc69f5b2dcc",
   "metadata": {},
   "source": [
    "**En dessous de 1, l'image s'éclaircit, en dessus de 1 elle s'assombrit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a8db6-98bc-4eab-9a82-fba67550b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_gamma = exposure.adjust_gamma(gray, gamma = 2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fd919-8c6b-4970-bc77-de3e06581461",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_gamma,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6652e27-0b95-40ce-8b64-d411b283c95d",
   "metadata": {},
   "source": [
    "**La correction fonctionne également sur des images couleurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfe364-6b99-4080-8f4a-6f04e6c53227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image couleur\n",
    "color_gamma = exposure.adjust_gamma(color, gamma = 2.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55a5d0-9032-41fd-9980-45cc52098d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en couleurs')\n",
    "ax1.imshow(color)\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(color_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd45fc-5410-4494-a643-e8302dd50875",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>L'outil est intéressant, enfin si l'on connait notre image et ses besoins.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3d95a-59e7-4783-b0b1-0feb59034475",
   "metadata": {},
   "source": [
    "### 1.2 Le contraste\n",
    "\n",
    "Le contraste définit la répartition de lumière dans l'image.\n",
    "Modifier le contraste de l'image permet d'ouvrir la fenêtre des pixels ; si les valeurs min et max ont peu d'écart, il est possible d'augmenter la rangée des valeurs utilisées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e58774-a883-42cf-a018-ac71d614a4ba",
   "metadata": {},
   "source": [
    "#### 1.2.1 Mofifier le contraste à la main\n",
    "\n",
    "Modifier le contraste c'est appliquer une transformation point par point. On peut le faire à la main en changeant chaque point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e986c4-9dfc-4c52-8bdc-e85258c325e1",
   "metadata": {},
   "source": [
    "#### Modifier le contraste des images en valeurs de gris à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e7a59-1f5f-4845-83d2-2d73cde14622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les valeurs min et max \n",
    "\n",
    "ma = gray.max()\n",
    "mi = gray.min()\n",
    "print(mi,ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65694c5-7e11-46de-b573-3132853ef921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir l'image en float et ouvrir la fenêtre de valeurs\n",
    "\n",
    "c = gray.astype(float)\n",
    "gray_c = 255.0*(c-mi)/(ma-mi+0.0000001).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84a00d-f5fd-44cb-a8a8-a29fa9c09761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Est-ce que ça a bien fonctionné ?\n",
    "\n",
    "ma1 = gray_c.max()\n",
    "mi1 = gray_c.min()\n",
    "print(mi1,ma1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f045f9a-d894-4b6e-8391-b519d8aa114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_c,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f744d8-c9e4-45f4-abc9-dab116425e77",
   "metadata": {},
   "source": [
    "#### Modifier le contraste des images en couleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b7381-9c90-47cf-8d80-388239d515e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On divise l'image en différents r, g, b \n",
    "\n",
    "r,g,b = cv2.split(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d1b18-c9a0-4337-8f48-6cc210532e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = r.max()\n",
    "mi = r.min()\n",
    "print(mi,ma)\n",
    "c = r.astype(float)\n",
    "im1r = 255.0*(c-mi)/(ma-mi+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a569c9-1df1-4fbc-b022-134cac7c568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = g.max()\n",
    "mi = g.min()\n",
    "print(mi,ma)\n",
    "c = g.astype(float)\n",
    "im1g = 255.0*(c-mi)/(ma-mi+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4b110-a106-46be-a911-cd79ba657576",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = b.max()\n",
    "mi = b.min()\n",
    "print(mi,ma)\n",
    "c = b.astype(float)\n",
    "im1b = 255.0*(c-mi)/(ma-mi+0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899ca02-3e9e-4460-ac17-44d2813bbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On remet les canaux ensemble\n",
    "# Attention à l'ordre des canaux de couleurs\n",
    "\n",
    "color_c = cv2.merge([im1b, im1g, im1r]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ba613-5978-41b7-a0ff-137bce729540",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image couleur')\n",
    "ax1.imshow(color)\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(color_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f95c4-b3f2-49d0-ac26-3d32fb7f9c1d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ces deux opérations sont là pour montrer qu'il est possible de modifier les valeurs de la matrice point par point.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d260040-2173-45db-82a9-0fcf2a504824",
   "metadata": {},
   "source": [
    "## **2. Les filtres et leurs applications**\n",
    "\n",
    "### 2.1 Qu'est-ce qu'un filtre? ?\n",
    "\n",
    "Dans le traitement numérique des images, les filtres sont utilisés principalement pour flouter, améliorer la netteté ou détecter les contours d'une image. Le filtre permet de supprimer les impurtées, le plus souvent il prépare l'image en vue d'opérations plus poussées, comme pour l'apprentissage profond. \n",
    "\n",
    "Un filtre est une petite matrice de dimension impair (3x3, 9x9, etc.) qui s'applique par convolution à l'image. Une convolution est simplement un opération matriciel entre le filtre et l'image. Le filtre (une matrice de 3x3 par exemple), se déplace sur l'image et une nouvelle image est obtenue lorsque l'opération est effectuée sur chaque pixel.\n",
    "L'opération de convolution réduit la taille de l'image, à moins que des bords soient ajoutés. \n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/0*YfpMfPnz6n2g4vIz.jpg\" title=\"filtre\"/>\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/William-Overell/publication/315478232/figure/fig30/AS:671530967113758@1537116867792/Concept-of-convolution-Our-kernel-mask-is-shown-below-as-M-From.ppm\" title=\"filtre2\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499d36b-7e60-494d-a498-253f0d18f557",
   "metadata": {},
   "source": [
    "### 2.2 Attention aux bords\n",
    "\n",
    "Le déplacement du filtre sur l'image pose la question du traitement des bords, car les pixels aux extrémités de l'image doivent avoir le même traitement que les autres ; c'est-à-dire qu'ils doivent passer par toutes les composantes du filtre. Dans ce cas, il y a plusieurs méthodes pour élargir les bords de l'image afin que tous les pixels de l'image de base soient traités correctement. \n",
    "\n",
    "Le processus de création de données en dehors de l'image s'appelle en anglais \"padding\".\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F3232%2F1*9reDuDh3nXs_kJ-M4eq0Ow.png&f=1&nofb=1&ipt=5e8431d61861a6afe5ece799945d9a7a3841a1e80ad8eb42c80e8c5b4f567755&ipo=images\" title=\"padding\"/>\n",
    "\n",
    "\n",
    "**Principalement, il y a**\n",
    "> * ajout de zéros\n",
    "> * constante arbitraire\n",
    "> * plus proche voisin\n",
    "> * en miroir \n",
    "> * reprend le bors opposé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729c3a2-357f-4c23-9ce0-3f1ca1222273",
   "metadata": {},
   "source": [
    "### 2.3 Du filtre à l'image\n",
    "\n",
    "Il existe plusieurs types de filtres dont le paramétrage et les effets sont bien connus. Avant de les aborder, regardons comment l'on peut simplement créer et appliquer un fitlre sur notre image.\n",
    "\n",
    "https://setosa.io/ev/image-kernels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d510a7d-7c79-473a-9bfc-0cec7e5e4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction convolve de la librairie Scipy nous permet d'opérer une convolution sur une image \n",
    "# Regardons ses paramètres\n",
    "\n",
    "import scipy.ndimage\n",
    "\n",
    "?scipy.ndimage.filters.convolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad3866-7d80-4962-9e9f-33c2eb24bb1b",
   "metadata": {},
   "source": [
    "**Il nous faut principalement une image (input), un filtre (weights) et un choix de padding (mode)**\n",
    "\n",
    "\n",
    "**À vous de faire**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a71349-2f4e-42f7-b956-b093eb7a3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un filtre, 3x3 par exemple\n",
    "\n",
    "filtre = np.array([[1,0,1],\n",
    "                  [0,1,0],\n",
    "                  [1,0,1]])\n",
    "print(filtre.shape, \"\\n\", filtre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dbf7ac-80a8-4e39-b2e7-b9757b6db941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrer la convolution\n",
    "\n",
    "grayf = gray.copy()\n",
    "\n",
    "gray_filtre = scipy.ndimage.filters.convolve(grayf, filtre, mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ed4f9-40a7-4ed7-a9bf-f01337067f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_filtre,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839626c-2369-46bc-87d6-e79bdf082f0d",
   "metadata": {},
   "source": [
    "Nous avons créé un filtre au hasard. Il existe des filtres bien connus.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Olivia-Cheung-2/publication/235368613/figure/fig1/AS:272536512495617@1441989176053/An-image-filtered-to-include-A-both-low-and-high-spatial-frequency-information-B.png\" title=\"freq1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357222d-3c10-473f-be69-b089bf5565eb",
   "metadata": {},
   "source": [
    "### 2.4 Des filtres pour réduire le bruit\n",
    "\n",
    "Egaliser un histogramme permet d'affiner le contraste d'une image en distribuant mieux les valeurs, de cette manière l'intensité est mieux répartie. Il existe plusieurs filtres bien connus dont:\n",
    "\n",
    "#### Filtre médian\n",
    "\n",
    "Le filtre médian permet de réduire le bruit tout en conservant les contours, il est souvent utilisé pour supprimer le bruit sel et poivre (incursion de pixels noirs et blancs dans l'image) ; chaque pixel est remplacé par la médiane de son voisinage et cela permet de supprimer les valeurs abberantes. Le filtre médian garde le contraste, la luminosité et les contours\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2Fdw60I.png&f=1&nofb=1&ipt=98a04cc296528ada2699bafee8ef3eafe1af5ab35f7a2d6743a7659fdbe48dc8&ipo=images\" title=\"median\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dad6a5-ab0e-47d1-a3f9-7ae2ed08aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "?scipy.ndimage.filters.median_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969754cd-42f9-48dd-a075-e2b155eafc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de la fonction\n",
    "# La fonction prend principalement en paramètres une image (input), la taille du filtre (size) et le type de padding (mode)\n",
    "\n",
    "gray_mm = gray.copy()\n",
    "\n",
    "gray_median = scipy.ndimage.filters.median_filter(gray_mm, size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc9c22-b7c4-477e-8051-8417990ee862",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_median,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c6882-5f7a-490d-bc84-8c25e62a6469",
   "metadata": {},
   "source": [
    "#### Filtre Gaussien\n",
    "\n",
    "Le filtre gaussien, comme son nom l'indique suit une distribution gaussienne, c'est-à-dire une loi normale centrée et réduite. Le sigma définit la forme de la cloche, et dans le traitement de l'image cela signifie que le bruit peut être réduit (sigma < 1) ou que le flou peut être accentué (sigma > 1).\n",
    "\n",
    "Le filtre gaussien vient lisser les imperfection de l'image, les détails et les contours sont atténués.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Generalized_normal_densities.svg/langfr-560px-Generalized_normal_densities.svg.png\" title=\"gauss\"/>\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.JpX8ONYKxZmIBAHIi1KpjAHaIy%26pid%3DApi&f=1&ipt=f751c30b941d608cdf6715c0829c2abc7c2d6f08aa83e0155fe4bef8ebc48ea9&ipo=images\" title=\"gauss2\"/>\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fsupport.cognex.com%2Fdocs%2Fcvl_900%2Fweb%2FEN%2Fcvl_vision_tools%2FContent%2FImages%2F18_8.jpg&f=1&nofb=1&ipt=0ba811ae846f092e40b20e9c7f1d74fc7f1e0f88240a48ec1ce7483ca4f1017b&ipo=images\" title=\"gauss3\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39353600-aa03-4844-b23a-c9e670732f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "?scipy.ndimage.filters.gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16dd1f-487b-4bdc-b257-db146ad890a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de la fonction\n",
    "# La fonction prend principalement une image (input) et un sigma\n",
    "\n",
    "gray_g = gray.copy()\n",
    "\n",
    "gray_gauss = scipy.ndimage.filters.gaussian_filter(gray_g, sigma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d0e78-6381-45c4-b156-5712a28d6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_gauss,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac570f9-45a1-4bba-a72b-be38c49415ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il existe également une fonction similaire dans la librairie OpenCV\n",
    "\n",
    "?cv2.GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e65b4f-e6de-4906-b68b-34446c6e5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de la fonction\n",
    "# La fonction prend principalement une image, une taille de filtre (ksize) et un sigma\n",
    "\n",
    "gray_gcv = gray.copy()\n",
    "\n",
    "gray_gausscv = cv2.GaussianBlur(gray_gcv, (19,19), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fedd135-0ea5-402e-85cc-a529ae078416",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_gausscv,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378801a3-3770-48b9-a984-c6efae77b631",
   "metadata": {},
   "source": [
    "**Pour aller plus loin: https://setosa.io/ev/image-kernels/**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05269ed0-6afe-4a71-94ff-051a2f29b86e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Exercice</b>: à partir de l'image et de la détection des visages ci-dessous, il vous faudra flouter ces visages et enregistrer l'image.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21c33c-700a-42a9-b15f-3c2d8f162b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# télécharger une image\n",
    "\n",
    "import requests \n",
    "\n",
    "DownURL = \"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fwww.bluebird-electric.net%2Facademia%2Facademia_pictures%2FUniversity_of_Geneva_Universite_de_Geneve_Switzerland_Planet_Solar.jpg&f=1&nofb=1&ipt=47cb55e9bf396b35de8f272546914dbe02979fdd2163fd7e2e15601bec35314c&ipo=images\" # choix de l'URL\n",
    "img_data = requests.get(DownURL).content # télécharger\n",
    "with open('unige.jpg', 'wb') as handler: # définir le fichier et son chemin\n",
    "    handler.write(img_data) # enregistrer l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1d272-bd02-47de-a564-a4742b9a9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import notre image\n",
    "\n",
    "path = \"unige.jpg\"\n",
    "image = cv2.imread(path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beceb027-2d4a-4f41-ab3b-6ab7e314c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer un algorithme simple\n",
    "# https://towardsdatascience.com/viola-jones-algorithm-and-haar-cascade-classifier-ee3bfb19f7d8\n",
    "\n",
    "grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\") #https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "detected_faces = face_cascade.detectMultiScale(grayscale_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb3054-795c-4d90-af2b-f08e1a17c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dessiner les lignes\n",
    "\n",
    "for (column, row, width, height) in detected_faces:\n",
    "    image = cv2.rectangle(image,(column, row),(column + width, row + height),(0, 255, 0),4)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912edeb-899c-4a9d-bf81-3e84d1905540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flouter les visages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc4bae4-70c4-486a-9c08-9a9633d68c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6d6c8-fac1-4445-8b69-7210b4977c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ef055-ca11-4e41-80b9-03a9437a6ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85064638-841b-42f0-8a13-320b4405f71e",
   "metadata": {},
   "source": [
    "### 2.5 Des filtres pour détecter les contours\n",
    "\n",
    "Dans une image, un contour se comprend comme la différence d'intensité entre deux pixels. La détection de contours cherche le changement soudain entre deux valeurs de pixel.\n",
    "\n",
    "Mathématiquement le calcul d'intensité se fait par l'utilisation de dérivées : en somme, l'application de une ou deux dérivées permet d'accentuer le contraste entre deux parties de l'image sensées décrire un contour. \n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi2.wp.com%2Fwww.adeveloperdiary.com%2Fwp-content%2Fuploads%2F2019%2F05%2FHow-to-implement-Sobel-edge-detection-using-Python-from-scratch-adeveloperdiary.com-sobel.jpg%3Fresize%3D600%252C281&f=1&nofb=1&ipt=429d15d804fa52c46efde3a1ffa086240b1a58a4d32969c28559b32185b03d89&ipo=images\" title=\"edge\"/>\n",
    "\n",
    "Il existe plusieurs types de filtres bien connus pour la détection des contours, nous allons en voir certains:\n",
    "\n",
    "* Prewitt\n",
    "* Sobel\n",
    "* Canny\n",
    "* Laplacien\n",
    "* Laplacien Gaussien\n",
    "\n",
    "\n",
    "Sans trop rentrer dans les détails, la détection de contours se divise principalement en deux méthodes. D'un côté une méthode basée sur le gradient via la première dérivée : le gradient est le taux de varaiation d'un point en fonction de ses voisins. D'un autre côté, une méthode basée sur une gaussienne à l'aide d'une derivée de second ordre. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43c1e2-b8b8-4135-ad36-627f15e67f99",
   "metadata": {},
   "source": [
    "#### Filtre Canny\n",
    "\n",
    "Le filtre Canny permet de détecter les contours (sans division horizontale et verticale préalable, comme le Sobel et Prewitt). Il est performant dans la détection (contours faibles et forts) et dans la localisation des contours (faible erreur entre contours détectés et contours réels). Malgré sa performance, son coup d'utilisation est non néglibeable.\n",
    "\n",
    "En d'autres termes, voilà le processus simplifié du filtre Canny:\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimage.slidesharecdn.com%2Fe2822eef-6993-4540-9321-65ca5f35eb39-161009120200%2F95%2Fexploring-methods-to-improve-edge-detection-with-canny-algorithm-13-638.jpg%3Fcb%3D1476014597&f=1&nofb=1&ipt=f300f199eaf34361e890d50c9054e9b38c9b25b29d6863942c7ec716bbdbc49b&ipo=images\" title=\"k-means2\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a082e7-9309-4b8a-9386-3efb34ca3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "?cv2.Canny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3871a48-ecc4-44de-898e-bc8b91bfe858",
   "metadata": {},
   "source": [
    "**La fonction prend principalement comme arguments: une image, un seuil bas et un seuil haut**\n",
    "\n",
    "> * En dessous du seuil bas, le pixel est rejeté\n",
    "> * En dessus du seuil haut, le pixel est considéré comme un contour\n",
    "> * Entre, si le pixel est connecté au seuil haut, il est accepté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34f600-af07-463c-8a2b-4de878750093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de la fonction\n",
    "\n",
    "gray_c = gray.copy()\n",
    "\n",
    "gray_canny = cv2.Canny(gray_c, 100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8c703-82e4-44f2-8d37-315969a4bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_canny,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec1128-3667-4114-a62e-d61bcd46a47c",
   "metadata": {},
   "source": [
    "#### Laplacien Gaussien\n",
    "\n",
    "Le filtre laplacien gaussien: la gaussienne est ajoutée au filtre laplacien afin de reduire le bruit dû à l'application du filtre par l'image. La méthode fonctionne particulièrement bien lorsque la transititon est abrupte. \n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fimage3.slideserve.com%2F5732879%2Flaplacian-of-gaussian-log-l.jpg&f=1&nofb=1&ipt=7de63d7d515aa0b0f2deab78c16be380dedb1cc5b2b9077ca489ce5c8171cfc5&ipo=images\" title=\"lg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99492a0-44ce-4fee-a2ec-f9a64798b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "?scipy.ndimage.filters.gaussian_laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a1472-5643-4b1b-914a-e5a35c205856",
   "metadata": {},
   "source": [
    "**La fonction prend principalement comme arguments: une image, un sigma et un padding**\n",
    "\n",
    "> * Le mode de padding, comme vu précédemment\n",
    "> * Le sigma fonctionne un peu comme un seuil, plus il est élevé plus on perd de points saillants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386a88d-0635-4291-817a-c3bb2c614cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramétrage de la fonction\n",
    "\n",
    "gray_lg = gray.copy()\n",
    "\n",
    "gray_laplace_gauss = scipy.ndimage.filters.gaussian_laplace(gray_lg, sigma=2, mode=\"reflect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdb76e-03e4-451e-bd39-58e40ed5802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,10))\n",
    "                                                 \n",
    "ax1.set_title ('Image en valeurs de gris')\n",
    "ax1.imshow(gray,cmap = \"gray\")\n",
    "\n",
    "ax2.set_title ('Image transformée')\n",
    "ax2.imshow(gray_laplace_gauss,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5fb8a-476d-4b93-8d9d-625897a64fe0",
   "metadata": {},
   "source": [
    "## 3. La détection sémantique grâce à l'apprentissage profond\n",
    "\n",
    "\n",
    "### 3.1 Qu'est-ce que l'apprentissage profond?\n",
    "\n",
    "L'apprentissage profond pour les images consiste à former des réseaux de neurones artificiels pour comprendre, reconnaître et extraire des informations à partir d'images. Ces réseaux apprennent à détecter des motifs visuels complexes, permettant par exemple la reconnaissance d'objets, la segmentation d'éléments dans une image ou encore la génération de contenu visuel.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi0.wp.com%2Fdevelopersbreach.com%2Fwp-content%2Fuploads%2F2020%2F08%2Fcnn_banner.png%3Ffit%3D1200%252C564%26ssl%3D1&f=1&nofb=1&ipt=210f2ef4bf2dc42949511533ba31820f74019d5bf5e49e6fc99d5dd612d5877e&ipo=images\" title=\"cnn\"/>\n",
    "\n",
    "\n",
    "Dans les réseaux de neurones convolutionnels, les filtres aident à détecter des motifs dans les images. Ils agissent comme des détecteurs de caractéristiques, captant des éléments tels que des bords, textures ou formes. En se superposant à l'image, ces filtres permettent au réseau de neurones d'extraire des informations clés pour reconnaître et interpréter son contenu.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.mdpi.com%2Finformation%2Finformation-10-00375%2Farticle_deploy%2Fhtml%2Fimages%2Finformation-10-00375-g002.png&f=1&nofb=1&ipt=2131ac2d9adeec1a01b6e21dd5aca4b2ffd370609b7a1f4bd8dbf238c7c5734b&ipo=images\" title=\"conv\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b610a-a1d4-4c45-a44e-28b45276edcb",
   "metadata": {},
   "source": [
    "### 3.2 Qu'est-ce qu'un algorithme d'apprentissage profond?\n",
    "\n",
    "Il existe plusieurs algorithmes d'apprentissage profond utilisés en traitement d'images. Parmi les plus populaires :\n",
    "\n",
    "> * Réseaux de neurones convolutionnels (CNN) : Les CNN utilisent des filtres pour extraire progressivement des caractéristiques des images, en passant de traits simples (comme les bords) à des attributs complexes pour la classification ou la détection d'objets.\n",
    "Ils sont spécifiquement conçus pour traiter des données structurées en grille comme les images. Les CNN sont largement utilisés pour la classification d'images, la détection d'objets, la segmentation sémantique et la génération d'images.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.F5A9npwC1qzLEn-rV59WlAHaDq%26pid%3DApi&f=1&ipt=b681764f327db8665c1f7bc338f6f0ae270e6d84572f2ba13082962b4d320968&ipo=images\" title=\"cnn2\"/>\n",
    "\n",
    "> * Réseaux antagonistes génératifs (GAN) : Les GAN consistent en deux réseaux adverses, un générateur créant des images et un discriminateur les évaluant ; ils s'améliorent mutuellement pour produire des images réalistes.\n",
    "Ils sont utilisés pour créer des images synthétiques réalistes et pour l'augmentation de données.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F8000%2F1*et3fMPDclTv6ZQSf1xbkag.jpeg&f=1&nofb=1&ipt=7e8b028485afc19c8094e8b345278b0c2ade5ea51ea58433a0f1f8cedd8c436d&ipo=images\" title=\"gan\"/>\n",
    "\n",
    "> * Réseaux neuronaux récurrents (RNN) : Bien que plus utilisés pour le traitement du langage naturel, les RNN utilisent des connexions cycliques pour traiter des données séquentielles comme des vidéos, prédisant ou générant des images basées sur des séquences précédentes.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1400%2F1*5bjD7kmtaJI-n3qztBC2Ig.png&f=1&nofb=1&ipt=4abd920e335f1c5f5da5ec46903edd21f74d2c661fdb1c9dc3566ad84c434804&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "\n",
    "#### En termes d'applications en traitement d'images :\n",
    "\n",
    "> * Classification d'images : Les CNN sont utilisés pour identifier et classer des objets dans des images, comme dans les applications de reconnaissance faciale, la détection de maladies à partir d'images médicales, etc.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F4096%2F1*nR5QCdmqUnvU2JFBu2Xa-Q.png&f=1&nofb=1&ipt=ea3c1fbb7dc192ac04b0a1fe2c485896118571d8ca24ba5b5c75b1e332fc3258&ipo=images\" title=\"ci\"/>\n",
    "\n",
    "> * Détection d'objets : Les CNN, notamment avec des architectures comme YOLO (You Only Look Once) ou SSD (Single Shot MultiBox Detector), sont employés pour localiser et identifier plusieurs objets dans une seule image.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fpjreddie.com%2Fmedia%2Fimage%2FScreen_Shot_2016-11-17_at_11.14.54_AM.png&f=1&nofb=1&ipt=bf73934fbefc8bb335cdbb110a4e97e3f91e6e7ea1e31908b88c6d45c4a9b7f0&ipo=images\" title=\"do\"/>\n",
    "\n",
    "> * Segmentation sémantique : Les réseaux tels que U-Net sont utilisés pour segmenter chaque pixel d'une image en catégories spécifiques, comme la délimitation des organes dans des images médicales.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.redd.it%2Fapse69opd3a61.jpg&f=1&nofb=1&ipt=4af80a8b267ff98907ed31358c760f7de9a04027d1658ccd6939241275e62f96&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "> * Génération d'images : Les GAN sont utilisés pour générer des images réalistes, comme la génération de visages, de paysages ou même de créations artistiques.\n",
    "\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fventurebeat.com%2Fwp-content%2Fuploads%2F2019%2F12%2F1_BU2GnLJF1AcrkhvbCHdppw-e1577390193648.jpeg%3Fw%3D1200%26strip%3Dall&f=1&nofb=1&ipt=67e8f68acbf4764ea54315b9863530bd32bac48a175fe184f8c9ab91aebc262f&ipo=images\" title=\"rnn\"/>\n",
    "\n",
    "\n",
    "Ces algorithmes sont appliqués dans de nombreux domaines, de la médecine à la vision par ordinateur en passant par la surveillance et la robotique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ed94f-b6dc-4943-8000-492196a501dc",
   "metadata": {},
   "source": [
    "### 3.3 Appliquer un algorithme d'apprentissage profond à une image\n",
    "\n",
    "Pour des raisons techniques nous allons nous déplacer sur Colab:\n",
    "\n",
    "https://colab.research.google.com/drive/11N8wwu2B9eXrV6gs8wNRq7X761CKEhvE?usp=share_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c111ad-f632-4950-b84f-aff07e91803c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
